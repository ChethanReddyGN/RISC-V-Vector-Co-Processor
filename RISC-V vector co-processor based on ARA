ARA ARCHITECTURE

1. Dispatcher and Sequencer:
    Dispatcher:
        •	Acts as the control unit of the coprocessor.
        •	Decodes incoming vector instructions dispatched by the core.
        •	Configures the necessary parameters such as vector length, data type, and mask information using Control and Status Registers (CSRs).
        •	Routes the vector instruction to appropriate subunits (e.g., lanes, VLSU, or SLDU).
    Sequencer:
        •	Controls the sequencing and scheduling of vector instructions within the lanes.
        •	Ensures that instructions are executed in an ordered manner, respecting data dependencies and synchronizing operations across lanes.

2. Vector Load/Store Unit (VLSU):
    The VLSU handles all memory operations for the vector coprocessor, including load and store operations.
    •	ADDRgen:
        o	Generates memory addresses for vector loads and stores.
        o	Handles memory alignment, stride calculations, and scatter-gather operations.
    •	VLDU (Vector Load Unit):
        o	Reads data from memory and loads it into the Vector Register File (VRF) for processing.
        o	Supports vectorized load operations, where multiple data elements are read in parallel.
    •	VSTU (Vector Store Unit):
        o	Writes vector data from the VRF to memory.
        o	Supports vectorized store operations, allowing efficient memory write-back.
    •	VRF Access:
        o	Interacts with the VRF to retrieve/store vector operands.
        o	Ensures alignment with memory and vector register indexing.

3. Slide Unit (SLDU):
    The SLDU handles data movement and reduction operations across lanes.
    •	Inter-lane communication:
        o	Slides vector data between lanes to facilitate operations such as reductions, shuffles, and shifts.
        o	Operates in logarithmic steps, reducing the overhead of inter-lane communication.
    •	Reduction Support:
        o	Implements reductions in a three-step process (intra-lane, inter-lane, and SIMD reduction).
        o	Minimizes latency by combining partial results efficiently across lanes.

4. Mask Unit (MASKU):
    The MASKU enables conditional execution of vector instructions based on vector masks.
    •	Vector Masking:
        o	Applies masks to vector elements to selectively enable or disable operations on specific elements.
        o	Useful for conditional computations and avoiding unnecessary operations on inactive elements.
    •	Support for RVV Mask Registers:
        o	Fully compliant with RVV mask registers and operations like AND, OR, NOT, etc.

5. Lanes:
    Lanes are the core computational units of the vector coprocessor. Each lane is an independent SIMD-like processing unit with the following components:
    •	Lane Sequencer:
        o	Orchestrates operations within the lane.
        o	Synchronizes with the main sequencer to execute assigned vector instructions.
    •	Operand Requester:
        o	Manages operand requests from the VRF.
        o	Ensures operands are available for vector functional units (VFUs) when needed.
    •	Vector Functional Units (VFUs):
        o	VALU (Vector Arithmetic Logic Unit): Performs integer and logical operations like addition, subtraction, and bitwise AND/OR.
        o	VMFPU (Vector Multi-Function Processing Unit): Handles floating-point vector operations like multiplication, division, and transcendental functions.
    •	Crossbars (xBar):
        o	Interconnects VFUs, operand queues, and VRF.
        o	Facilitates high-speed data transfer and resource sharing within the lane.
    •	SRAM Banks:
        o	Each lane has multiple SRAM banks for local storage, enabling parallel access and reducing VRF contention.

6. Vector Register File (VRF):
    The VRF is a shared resource that stores vector operands and results.
    •	Partitioned Design:
        o	Organized into chunks and distributed across lanes.
        o	Each lane can access its dedicated chunk for reduced latency.
    •	SRAM Implementation:
        o	High-speed SRAM ensures low latency for read/write operations.
        o	Supports multiple read/write ports for parallel access by lanes and functional units.

7. Reduction Support:
    Reduction operations aggregate data across vector elements to produce a single result.
    •	Three-Step Reduction Process:
        1.	Intra-lane Reduction: Combines elements within each lane.
        2.	Inter-lane Reduction: Aggregates partial results across lanes using the SLDU.
        3.	SIMD Reduction: Performs final reduction within a SIMD word if needed.
    •	Efficiency:
        o	Logarithmic latency for inter-lane communication.
        o	Maximizes ALU utilization during intra-lane reduction.

8. AXI Invalidation Filter
    Ensures memory consistency and coherence between the core and the coprocessor
    •	Coherence Signals:
        o	Synchronizes scalar (CVA6) and vector memory accesses.
        o	Invalidates stale cache lines in the scalar L1 cache.
    •	Memory Interconnect:
        o	Shared memory interface for scalar and vector memory operations.

9. Communication Between Core and Coprocessor
    The CVA6 core and ARA coprocessor communicate through well-defined interfaces:
    •	Instruction Dispatch:
        o	The core dispatches vector instructions to the dispatcher in ARA.
    •	Result Communication:
        o	The coprocessor writes results back to the VRF or memory, which the core can access.
    •	Status and Synchronization:
        o	Synchronization signals ensure scalar and vector operations progress without conflicts.


VRF ACCESSSING:

1. Address Generation and Data Access
    •	The ADDRgen (Address Generator) module in the VLSU calculates the memory addresses for each vector element based on the base address, stride, and other parameters.
    •	For vector loads, data is fetched from memory using these addresses and written into the VRF.
    •	For vector stores, data is read from the VRF and written back to memory.

2. Accessing the VRF
    The VLSU accesses the VRF through a well-defined interface. Here's how it works:
    1.	Memory Alignment and Addressing:
        o	The VRF is divided into chunks, with each lane responsible for a portion of the VRF.
        o	The VLSU determines which lane holds the data based on the vector element index and routes the access request accordingly.
    2.	Parallel Access:
        o	The VRF supports multiple read/write ports to allow simultaneous access by multiple units, including the VLSU and other functional units (e.g., VFUs in lanes).
        o	The VLSU can access multiple vector elements in parallel, leveraging the parallelism of the VRF structure.
    3.	Crossbar Interconnect:
        o	Inside each lane, a crossbar (xBar) interconnect allows routing between the VRF chunks and the functional units, including the VLSU.
        o	The VLSU interacts with the crossbar to read/write data to the VRF efficiently.
    4.	 Data Transfer:
        o	For loads, data fetched from memory is written into the appropriate chunk of the VRF.
        o	For stores, the VLSU reads data from the VRF and transfers it to memory.
        o	The VRF ensures low-latency data transfer by utilizing high-speed SRAM with parallel read/write capabilities.

3. Managing VRF Access Conflicts
    •	Since the VRF is a shared resource accessed by multiple units (e.g., lanes, MASKU, SLDU), an arbiter resolves access conflicts.
    •	The arbiter prioritizes requests based on predefined policies (e.g., instruction order, functional unit type).

4. Data Alignment and Masking
    •	The VLSU ensures proper alignment of vector elements when writing to or reading from the VRF.
    •	Masks generated by the MASKU can selectively enable or disable elements during load/store operations, allowing for conditional memory accesses.

5. Integration with AXI Memory Interface
    •	The VLSU operates in tandem with the AXI memory interface to fetch/store vector data from main memory.
    •	Data is transferred between the VLSU and VRF at high bandwidth to maximize throughput.

Key Features of VLSU-VRF Access:
    •	Parallelism: Exploits the distributed nature of the VRF to perform multiple accesses simultaneously.
    •	Efficiency: The crossbar interconnect and chunked VRF design reduce latency and maximize throughput.
    •	Scalability: The architecture scales efficiently with the number of lanes, as each lane operates on its own chunk of the VRF.



TOOL CHAINS:

1. Newlib: Newlib is a C standard library implementation designed for use on embedded systems. It provides routines for handling standard input/output operations, memory allocation, string manipulation, and other functionalities necessary for C programming, particularly in environments with limited resources. It's commonly used with GCC (GNU Compiler Collection) toolchains to support application development on various platforms.

2. RISC-V GNU: The  RISC-V GNU Toolchain is a collection of programming tools for developing software for RISC-V architecture. It includes a compiler (like GCC), assembler, linker, and debugger, tailored for RISC-V. The toolchain provides developers with the necessary tools to write, compile, and debug RISC-V programs efficiently.
GitHub - riscv-collab/riscv-gnu-toolchain: GNU toolchain for RISC-V, including GCC

3.RISC-V ISA sim: A RISC-V ISA simulation toolchain typically includes a variety of tools for software development, simulation, and debugging for RISC-V processors. Key components often consist of a simulator (like Spike), a GNU toolchain (such as GCC compiled for RISC-V), and debugging tools (like GDB). This setup enables developers to compile, simulate, and debug programs targeting RISC-V architectures efficiently.
GitHub - riscv-software-src/riscv-isa-sim: Spike, a RISC-V ISA Simulator

4. RISC-V LLVM: The RISC-V LLVM toolchain is a set of compilation tools specifically designed to support the RISC-V architecture using the LLVM framework. It includes a compiler, assembler, linker, and debugger optimized for RISC-V, allowing developers to build software for RISC-V hardware efficiently. You can find pre-built binaries or build the toolchain from source, which is often recommended for the latest features and optimizations.
GitHub - andestech/riscv-llvm-toolchain

5. Verilator: Verilator is a tool for converting Verilog code into C++ or SystemC code to facilitate simulation and testing of digital systems. It is widely used in hardware design verification due to its performance advantages over traditional interpreters. Verilator supports various SystemVerilog constructs and can optimize the generated code for improved simulation speed.


SLDU:
    The SLDU (Slide Load Unit) in some vector processor architectures, like the one you're describing, is responsible for handling instructions that must access all Vector Register File (VRF) banks at once. This typically happens in operations that involve vectorized reductions, parallel processing, or operations that require simultaneous access to multiple vector registers across different VRF banks.
    Function of the SLDU in This Context:
        •	Accessing All VRF Banks: The SLDU is designed to efficiently access all banks of the VRF simultaneously. This is essential for operations that need to process entire vectors or multiple elements in parallel, especially when the data is spread across multiple VRF banks.
        •	Parallel Processing: When performing parallel operations, such as reduction (e.g., summing all elements of a vector or computing a dot product), the SLDU ensures that the data is fetched in parallel from all VRF banks to provide the needed operands to functional units (like VALU, VMUL, VFPU).
        •	Reduction Operations: The SLDU is often used for reduction operations, which typically require accessing all elements of a vector (or multiple vectors) at once, processing them in parallel, and then applying a reduction operator (e.g., sum, max, average, etc.).
    Architectural Overview of SLDU's Role:
    In this architecture, the SLDU:
        1.	Operates in Parallel: It is responsible for fetching operands from all VRF banks simultaneously, enabling the parallel processing of vectors.
        2.	Handles Reduction Instructions: It supports operations that require accessing and reducing the values from all vector elements in the VRF at once.
        3.	Coordinates with Other Units: It works closely with other functional units and components (like the Operand Request Unit, Crossbars, and Arbiter) to ensure efficient data access, scheduling, and result storage.
    Example Use Cases of SLDU:
        •	Reduction across Multiple Vectors: For example, summing all elements of a vector (or multiple vectors) that are distributed across VRF banks.
        •	Max/Min Operations: Accessing all vector elements to compute the maximum or minimum value across a vector in parallel.
        •	Dot Product Computation: Fetching multiple vector elements across banks and performing a dot product operation.
